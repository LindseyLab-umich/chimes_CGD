{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.spatial.distance import cdist\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58,)\n",
      "0     25.0\n",
      "1     25.0\n",
      "2     25.0\n",
      "3     25.0\n",
      "4     25.0\n",
      "5     25.0\n",
      "6     25.0\n",
      "7     25.0\n",
      "8     25.0\n",
      "9     25.0\n",
      "10    25.0\n",
      "11    25.0\n",
      "12     5.0\n",
      "13     5.0\n",
      "14    20.0\n",
      "15    20.0\n",
      "16    20.0\n",
      "17     2.0\n",
      "18    20.0\n",
      "19    20.0\n",
      "20    14.0\n",
      "21    20.0\n",
      "22     4.0\n",
      "23    20.0\n",
      "24     2.0\n",
      "25    20.0\n",
      "26    20.0\n",
      "27    20.0\n",
      "28    20.0\n",
      "29    20.0\n",
      "30    20.0\n",
      "31    20.0\n",
      "32    20.0\n",
      "33    20.0\n",
      "34    20.0\n",
      "35    20.0\n",
      "36    20.0\n",
      "37    20.0\n",
      "38    20.0\n",
      "39    20.0\n",
      "40    20.0\n",
      "41    20.0\n",
      "42    20.0\n",
      "43    20.0\n",
      "44    20.0\n",
      "45    20.0\n",
      "46    20.0\n",
      "47     4.0\n",
      "48    20.0\n",
      "49    20.0\n",
      "50    20.0\n",
      "51    20.0\n",
      "52    20.0\n",
      "53    20.0\n",
      "54    20.0\n",
      "55    20.0\n",
      "56    20.0\n",
      "57    20.0\n",
      "Name: Total frames, dtype: float64\n",
      "(1116,)\n",
      "1116.0\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Carbon_data_defs.csv', header=1)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "tot_frames = df[\"Total frames\"]\n",
    "tot_frames = tot_frames.dropna()\n",
    "print(np.shape(tot_frames))\n",
    "print(tot_frames)\n",
    "\n",
    "# Initialize an empty list to store the result\n",
    "alc_labels = []\n",
    "\n",
    "# Iterate through the array of integers\n",
    "for index, value in enumerate(tot_frames):\n",
    "    # Duplicate the index based on the value of the element\n",
    "    for _ in range(int(value)):\n",
    "        alc_labels.append(index)\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = \"alc_labels\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(alc_labels, pickle_file)\n",
    "\n",
    "print(np.shape(alc_labels))\n",
    "print(sum(tot_frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/blaubach/chimes_CGD-myLLFork/dissimilarity_clustering/alc_pd_2bS\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 0\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 1\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 2\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 3\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 4\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 5\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 6\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 7\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 8\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 9\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 10\n",
      "Number of Frames in SP: 25\n",
      "Completed SP 11\n",
      "Number of Frames in SP: 5\n",
      "Completed SP 12\n",
      "Number of Frames in SP: 5\n",
      "Completed SP 13\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 14\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 15\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 16\n",
      "Number of Frames in SP: 2\n",
      "Completed SP 17\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 18\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 19\n",
      "Number of Frames in SP: 14\n",
      "Completed SP 20\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 21\n",
      "Number of Frames in SP: 4\n",
      "Completed SP 22\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 23\n",
      "Number of Frames in SP: 2\n",
      "Completed SP 24\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 25\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 26\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 27\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 28\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 29\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 30\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 31\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 32\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 33\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 34\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 35\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 36\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 37\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 38\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 39\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 40\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 41\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 42\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 43\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 44\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 45\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 46\n",
      "Number of Frames in SP: 4\n",
      "Completed SP 47\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 48\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 49\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 50\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 51\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 52\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 53\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 54\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 55\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 56\n",
      "Number of Frames in SP: 20\n",
      "Completed SP 57\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_files(path, specific_chunk_index, file_chunks, mode=\"all\"):\n",
    "\n",
    "    # Choose a specific chunk\n",
    "    specific_chunk = file_chunks[specific_chunk_index]\n",
    "    print(f\"Number of Frames in SP: {len(specific_chunk)}\")\n",
    "\n",
    "    # Determine which files to consider based on the mode\n",
    "    if mode == \"equilibrium\":\n",
    "        start_index = len(specific_chunk) // 2\n",
    "        files_to_process = specific_chunk[start_index:]\n",
    "    elif mode == \"all\":\n",
    "        files_to_process = specific_chunk\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Use 'equilibrium' or 'all'.\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Process the chosen files\n",
    "    for file_name in files_to_process:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        #print(file_path)\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            #print(f\"Processing {mode} files - Normalized second column entries of {file_name}:\")\n",
    "            second_column_entries = []\n",
    "            for line in lines[:60]:\n",
    "                columns = line.split()\n",
    "                if len(columns) >= 2:\n",
    "                    second_column_entry = float(columns[1])\n",
    "                    second_column_entries.append(second_column_entry)\n",
    "            all_data.append(second_column_entries)\n",
    "    #print(np.shape(all_data))\n",
    "    return all_data\n",
    "\n",
    "# Example usage:\n",
    "#os.chdir('..')\n",
    "current_directory = os.getcwd()\n",
    "#print(\"Current Directory:\", current_directory)\n",
    "\n",
    "body = 2\n",
    "path_2b = f\"/alc_pd_{body}bS\"  # Replace this with your actual path\n",
    "path = current_directory + path_2b\n",
    "print(path)\n",
    "\n",
    "# Choose a specific chunk (e.g., Chunk_0) and mode ('equilibrium' or 'all')\n",
    "# specific_chunk_index = 0\n",
    "mode = \"equilibrium\"  # Change this to 'all' if needed\n",
    "all_avgs_equilibrium = []\n",
    "all_data = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Sort the filtered files based on file names\n",
    "sorted_files = sorted(files)\n",
    "# Initialize an empty list to store sublists (chunks) of files\n",
    "file_chunks = []\n",
    "\n",
    "# Initialize the starting index for slicing the sorted files list\n",
    "start_index = 0\n",
    "\n",
    "# Iterate over each chunk size\n",
    "for size in tot_frames:\n",
    "    size = int(size)\n",
    "    # Determine the end index for slicing\n",
    "    end_index = start_index + size\n",
    "    \n",
    "    # Slice the sorted files list to create a chunk\n",
    "    chunk = sorted_files[start_index:end_index]\n",
    "    \n",
    "    # Append the chunk to the list of file chunks\n",
    "    file_chunks.append(chunk)\n",
    "    \n",
    "    # Update the start index for the next chunk\n",
    "    start_index = end_index\n",
    "\n",
    "# Print the list of file chunks\n",
    "#print(file_chunks)\n",
    "\n",
    "for specific_chunk_index in range(len(tot_frames)):\n",
    "\n",
    "    data = process_files(path, specific_chunk_index, file_chunks, mode)\n",
    "    all_data.append(data)\n",
    "    #print(np.shape(all_data))\n",
    "    #print(np.shape(data))\n",
    "    avg_data = np.mean(data, axis = 0)\n",
    "    avg_data = avg_data.reshape(-1,1)\n",
    "    print(f\"Completed SP {specific_chunk_index}\")\n",
    "    all_avgs_equilibrium.append(avg_data)\n",
    "    #print(np.shape(all_avgs_equilibrium))\n",
    "\n",
    "# print(\"All data:\" ,np.shape(all_data))\n",
    "# print(\"Avg. data:\", np.shape(all_avgs_equilibrium))\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = f\"{body}b_alc_pd_{mode}.ary\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(all_data, pickle_file)\n",
    "\n",
    "# Write the object to a pickle file\n",
    "pickle_filename = f\"{body}b_alc_avg_pd_{mode}.ary\"\n",
    "with open(pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(all_avgs_equilibrium, pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
