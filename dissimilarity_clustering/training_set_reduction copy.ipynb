{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/blaubach/chimes_CGD-myLLFork/dissimilarity_clustering\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_#0000.xyz\n",
      "training_data_#0050.xyz\n",
      "training_data_#0055.xyz\n",
      "training_data_#0060.xyz\n",
      "training_data_#0075.xyz\n",
      "training_data_#0080.xyz\n",
      "training_data_#0110.xyz\n",
      "(300, 180)\n",
      "       2B_0      2B_1      2B_2      2B_3      2B_4      2B_5      2B_6  \\\n",
      "0  0.151940  0.079974  0.039987  0.134957  0.000000  0.039987  0.000000   \n",
      "1  0.151838  0.089548  0.030670  0.107863  0.026939  0.039628  0.000530   \n",
      "2  0.152059  0.091794  0.027968  0.104630  0.029103  0.039305  0.001128   \n",
      "3  0.113888  0.093885  0.074372  0.063481  0.048075  0.037675  0.033869   \n",
      "4  0.118745  0.094125  0.073008  0.061039  0.048168  0.036493  0.032401   \n",
      "\n",
      "       2B_7      2B_8      2B_9  ...  4B_54  4B_55  4B_56  4B_57  4B_58  \\\n",
      "0  0.059981  0.000000  0.064979  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "1  0.059527  0.008709  0.053400  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "2  0.058054  0.005966  0.057973  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "3  0.031662  0.025958  0.021373  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "4  0.030655  0.026413  0.021867  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   4B_59  labels      Pavg      Pstd  Natoms  \n",
      "0    0.0       0  0.016335  0.025764     216  \n",
      "1    0.0       0  0.016335  0.024411     216  \n",
      "2    0.0       0  0.016335  0.024316     216  \n",
      "3    0.0       0  0.016348  0.020050     216  \n",
      "4    0.0       0  0.016344  0.020228     216  \n",
      "\n",
      "[5 rows x 184 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Define file paths with cwd appended\n",
    "file_path_2b = os.path.join(cwd, \"dft_pds/2b_all_pd\")\n",
    "file_path_3b = os.path.join(cwd, \"dft_pds/3b_all_pd\")\n",
    "file_path_4b = os.path.join(cwd, \"dft_pds/4b_all_pd\")\n",
    "file_path_labels = os.path.join(cwd, \"dft_pds/labels_pd\")\n",
    "file_path_natoms = os.path.join(cwd, \"test_notebooks/energies_per_atom.txt\")\n",
    "\n",
    "# Open pickle files with the updated file paths\n",
    "with open(file_path_2b, 'rb') as pickle_file:\n",
    "    pd_2b = pickle.load(pickle_file)\n",
    "\n",
    "with open(file_path_3b, 'rb') as pickle_file:\n",
    "    pd_3b = pickle.load(pickle_file)\n",
    "\n",
    "with open(file_path_4b, 'rb') as pickle_file:\n",
    "    pd_4b = pickle.load(pickle_file)\n",
    "\n",
    "with open(file_path_labels, 'rb') as pickle_file:\n",
    "    labels = pickle.load(pickle_file)\n",
    "\n",
    "natom_list = []\n",
    "\n",
    "# Open the text file for reading\n",
    "with open(file_path_natoms, 'r') as file:\n",
    "\n",
    "    # Read the contents of the file\n",
    "    lines = file.readlines()[1:]\n",
    "\n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "\n",
    "        # Split the line into words\n",
    "        words = line.split()\n",
    "        natoms = line.split(\"|\")[1].strip()\n",
    "\n",
    "        # Extract the last word, assuming it's a number\n",
    "        last_number = float(words[-1])\n",
    "        if words[0][-1] == 'z':\n",
    "            print(words[0])\n",
    "            continue\n",
    "        natom_list.append(natoms)\n",
    "\n",
    "natom_list = np.array(natom_list[:-10])\n",
    "\n",
    "# Combine the arrays along the second axis (axis=1)\n",
    "all_array = np.concatenate((pd_2b, pd_3b, pd_4b), axis=2)\n",
    "all_array = all_array.reshape(-1, all_array.shape[2])\n",
    "print(np.shape(all_array))\n",
    "df_fingerprints = pd.DataFrame(all_array)\n",
    "\n",
    "# Define the column labels for each set of columns\n",
    "column_labels_2b = [f'2B_{i}' for i in range(60)]\n",
    "column_labels_3b = [f'3B_{i}' for i in range(60)]\n",
    "column_labels_4b = [f'4B_{i}' for i in range(60)]\n",
    "\n",
    "# Assign the column labels to the DataFrame\n",
    "column_labels = column_labels_2b + column_labels_3b + column_labels_4b\n",
    "df_fingerprints.columns = column_labels\n",
    "\n",
    "# Add a new column \"labels\" to the DataFrame and assign the new vector to it\n",
    "df_fingerprints['labels'] = labels\n",
    "\n",
    "# Calculate the row-wise mean using `mean(axis=1)`\n",
    "row_avg = df_fingerprints.mean(axis=1)\n",
    "\n",
    "# Append the calculated row-wise mean as a new column named \"Pavg\"\n",
    "df_fingerprints['Pavg'] = row_avg\n",
    "\n",
    "# Calculate the row-wise standard deviation using `std(axis=1)`\n",
    "row_std = df_fingerprints.std(axis=1)\n",
    "\n",
    "# Append the calculated row-wise standard deviation as a new column named \"Pstd\"\n",
    "df_fingerprints['Pstd'] = row_std\n",
    "\n",
    "# Append Natoms to Dataframe\n",
    "df_fingerprints['Natoms'] = natom_list\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(df_fingerprints.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 25, 184)\n"
     ]
    }
   ],
   "source": [
    "# Define the size of each chunk\n",
    "chunk_size = 25\n",
    "\n",
    "# Initialize an empty list to store the chunks\n",
    "chunk_list = []\n",
    "\n",
    "# Iterate over the rows of df_fingerprints in chunks of size chunk_size\n",
    "for i in range(0, len(df_fingerprints), chunk_size):\n",
    "    # Select a chunk of rows\n",
    "    chunk = df_fingerprints.iloc[i:i+chunk_size]\n",
    "    \n",
    "    # Assign the chunk to a variable with a dynamic name (e.g., sp1, sp2, ...)\n",
    "    globals()[f'sp{i//chunk_size + 1}'] = chunk\n",
    "    \n",
    "    # Append the chunk to the list\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "# Now, sp1, sp2, ..., will contain the first 25, next 25, ..., rows of df_fingerprints\n",
    "print(np.shape(chunk_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the minimum occurrence count required\n",
    "def compute_pij(phi1, phi2):\n",
    "\n",
    "    # Compute statistics for phi1\n",
    "    concatenated_df = pd.concat([df.iloc[:-4] for df in phi1], axis=1)\n",
    "    average = concatenated_df.mean()\n",
    "    std_dev = concatenated_df.std()\n",
    "    natom_list = []\n",
    "    for df in phi1:\n",
    "        natom_list.append(int(df.iloc[-1]))\n",
    "    natoms_phi1 = np.mean(natom_list)\n",
    "\n",
    "    # Compute statistics for phi2\n",
    "    phi2_fingerprint = phi2.iloc[:-4]\n",
    "    avg_phi2 = phi2.iloc[-3]\n",
    "    std_phi2 = phi2.iloc[-2]\n",
    "    natoms_phi2 = float(phi2.iloc[-1])\n",
    "\n",
    "    # Standardize Natoms\n",
    "    natoms = np.sqrt(natoms_phi1*natoms_phi2) # Modification to Pij from BL\n",
    "\n",
    "    # Compute Pij\n",
    "    diff_phi1 = np.sum(concatenated_df-average)\n",
    "    diff_phi2 = np.sum(phi2_fingerprint-avg_phi2)\n",
    "    numerator = diff_phi1*diff_phi2\n",
    "    denominator = natoms*std_dev*std_phi2\n",
    "    pij = numerator/denominator\n",
    "\n",
    "    return float(pij.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 184)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Index of the row with the minimum sum: 227\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'configuration_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg_cnt):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 81\u001b[0m construct_comparison(training_set, available_configurations)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mconstruct_comparison\u001b[0;34m(training_set, candidates)\u001b[0m\n\u001b[1;32m     42\u001b[0m min_pij_index \u001b[38;5;241m=\u001b[39m sums\u001b[38;5;241m.\u001b[39midxmin()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex of the row with the minimum sum:\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_pij_index)\n\u001b[0;32m---> 46\u001b[0m update_training, update_configurations, update_pij_matrix \u001b[38;5;241m=\u001b[39m update_configuration_sets(min_pij_index, training_set, configuration_set, pij_matrix)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m update_training, update_configurations, update_pij_matrix\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configuration_set' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the minimum occurrence count required\n",
    "def compute_pij(ph1, phi2):\n",
    "\n",
    "    # Compute statistics for training set\n",
    "    phi1_fingerprint = phi2.iloc[:-4]\n",
    "    avg_phi1 = phi2.iloc[-3]\n",
    "    std_phi1 = phi2.iloc[-2]\n",
    "    natoms_phi1 = float(phi2.iloc[-1])\n",
    "\n",
    "    # Compute statistics for phi2\n",
    "    phi2_fingerprint = phi2.iloc[:-4]\n",
    "    avg_phi2 = phi2.iloc[-3]\n",
    "    std_phi2 = phi2.iloc[-2]\n",
    "    natoms_phi2 = float(phi2.iloc[-1])\n",
    "\n",
    "    # Standardize Natoms\n",
    "    natoms = np.sqrt(natoms_phi1*natoms_phi2)\n",
    "\n",
    "    # Compute pij\n",
    "    numerator = np.sum((phi1_fingerprint-avg_phi1)*(phi2_fingerprint-avg_phi2))\n",
    "    denominator = natoms*std_phi1*std_phi2\n",
    "    pij = numerator/denominator\n",
    "\n",
    "    return pij\n",
    "\n",
    "def construct_comparison(training_set, candidates):\n",
    "\n",
    "    column_values = training_set.index.tolist()\n",
    "    pij_matrix = pd.DataFrame(columns = column_values)\n",
    "    rowIndex = 0\n",
    "    pij_values = []\n",
    "    for index_ts, phi1 in training_set.iterrows():\n",
    "        for index_ac, phi2 in candidates.iterrows():\n",
    "            pij_values.append(compute_pij(phi1, phi2))\n",
    "        pij_matrix[column_values[rowIndex]] = pij_values\n",
    "        pij_values = []\n",
    "        rowIndex += 1\n",
    "    # Calculate the sum of values in each row and store the results in a list\n",
    "    sums = pij_matrix.sum(axis=1)\n",
    "\n",
    "    # Find the index of the minimum value in the sums list\n",
    "    min_pij_index = sums.idxmin()\n",
    "\n",
    "    print(\"Frame with the minimum Pij sum:\", min_pij_index)\n",
    "\n",
    "    update_training, update_configurations, update_pij_matrix = update_configuration_sets(min_pij_index, training_set, configuration_set, pij_matrix)\n",
    "    \n",
    "    return update_training, update_configurations, update_pij_matrix\n",
    "\n",
    "def update_configuration_sets(min_pij_index, training_set, configuration_set, pij_matrix):\n",
    "\n",
    "    training_set = pd.concat([training_set, pd.DataFrame([pij_matrix.iloc[min_pij_index]])])\n",
    "    pij_matrix = pij_matrix.drop(min_pij_index)\n",
    "    configuration_set = configuration_set.drop(min_pij_index)\n",
    "    return training_set, configuration_set, pij_matrix\n",
    "\n",
    "print(np.shape(df_fingerprints))\n",
    "\n",
    "# Choose arbitrary starting point\n",
    "starting_frame = 0\n",
    "phi1 = df_fingerprints.iloc[starting_frame]\n",
    "phi1_fingerprint = phi1.iloc[:-4]\n",
    "avg_phi1 = phi1.iloc[-3]\n",
    "std_phi1 = phi1.iloc[-2]\n",
    "natoms_phi1 = float(phi1.iloc[-1])\n",
    "\n",
    "# Initialize training set\n",
    "training_set = pd.DataFrame(columns=df_fingerprints.columns)\n",
    "training_set = pd.concat([training_set, pd.DataFrame([phi1])])\n",
    "\n",
    "# Initialize values:\n",
    "min_pij = np.inf\n",
    "\n",
    "available_configurations = copy.copy(df_fingerprints)\n",
    "available_configurations = available_configurations.drop(available_configurations.index[starting_frame])\n",
    "\n",
    "# Compute pij matrix\n",
    "cfg_cnt = 5\n",
    "for i in range(cfg_cnt):\n",
    "    construct_comparison(training_set, available_configurations)\n",
    "# print(f\"Adding Frame {min_row_index} with Pij {min_pij}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
